---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: kube-prometheus-stack
spec:
  interval: 15m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 77.14.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
      interval: 15m
  timeout: 20m
  maxHistory: 3
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    crds:
      enabled: true
      upgradeJob:
        enabled: true
        forceConflicts: true
    defaultRules:
      create: true
      rules:
        alertmanager: false # unlikely errors in 1 replica setup
        etcd: false
        general: false # no watchdog/scrape failure IDEALLY SET THIS UP
        k8s: true
        k8sContainerMemorySwap: false # dropped container_memory_swap
        k8sContainerMemoryCache: true # used in node compute dashboard
        k8sPodOwner: true # used in good dashboards
        kubeApiserver: false
        kubeApiserverAvailability: false
        kubeApiserverError: false
        kubeApiserverSlos: false
        kubeControllerManager: false
        kubelet: false # dumb pleg rules
        kubePrometheusGeneral: false # not using up1 / up0 anyway
        kubePrometheusNodeRecording: false # for node overview dashboards
        kubernetesApps: true
        kubernetesResources: false # don't care about quotas atm
        kubernetesStorage: false # pvc warnings are good, but local-provisioner do not support them yet
        kubernetesSystem: false
        kubeScheduler: false
        kubeSchedulerAlerting: false
        kubeStateMetrics: true
        network: false # don't care bout network flaps atm
        node: false # don't need these to visualise them - default boards are bad
        nodeExporterAlerting: false
        nodeExporterRecording: false
        prometheus: true # nice safety
        prometheusOperator: true # nice safety when not using admission webhook
        windows: false # wat
      disabled:
        # some of the heavier prom alerts that we don't need
        PrometheusNotIngestingSamples: true
        PrometheusRemoteStorageFailures: true
        PrometheusRemoteWriteDesiredShards: true
        PrometheusRemoteWriteBehind: true
    cleanPrometheusOperatorObjectNames: true
    prometheusOperator:
      enabled: true
      kubeletService:
        enabled: enabled
        namespace: kube-system
      serviceMonitor:
        selfMonitor: true
        metricRelabelings:
        # we can look at this if things start to fail don't need histograms
        - sourceLabels: [__name__]
          regex: 'prometheus_operator_(kubernetes_client|reconcile_duration).*'
          action: drop
        # we are not debugging go services
        - sourceLabels: [__name__]
          regex: 'go_sched_.*_bucket'
          action: drop
      revisionHistoryLimit: 4
    windowsMonitoring:
      enabled: false
    prometheus-windows-exporter:
      prometheus:
        monitor:
          enabled: false
    nodeExporter:
      enabled: true
      jobLabel: job
      operatingSystems:
        darwin:
          enabled: false
    prometheus-node-exporter:
      fullnameOverride: node-exporter
      podLabels:
        job: node-exporter
      prometheus:
        monitor:
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node
          metricRelabelings:
            # we are not debugging go services
            - sourceLabels: [__name__]
              regex: '^go_.*'
              action: drop
            # drop some we don't use / need
            - sourceLabels: [__name__]
              regex: '^node_cpu_guest_seconds_total'
              action: drop
            - sourceLabels: [__name__]
              regex: '^node_network_(flags|device_id|dormant|iface_link_mode|transmit_queue_length|carrier.*)'
              action: drop
            - sourceLabels: [__name__]
              regex: '^node_scrape_(collector_duration_seconds)'
              action: drop
            - sourceLabels: [__name__]
              regex: '^node_filesystem_(readonly|device_error|files).*'
              action: drop
            - sourceLabels: [__name__]
              regex: '^node_(cooling|schedstat|cpu_scaling).*'
              action: drop
  ## Configuration for kube-state-metrics subchart
    kube-state-metrics:
      #extraArgs: ["-v"]
      # Extra rbac for flux metrics
      rbac:
        create: true
        # For customResourceState for flux metrics
        # https://fluxcd.io/flux/monitoring/custom-metrics/
        extraRules:
        - apiGroups: ["kustomize.toolkit.fluxcd.io"]
          resources: ["kustomizations"]
          verbs: ["list", "watch"]
        - apiGroups: ["source.toolkit.fluxcd.io"]
          resources: ["gitrepositories", "ocirepositories"]
          verbs: ["list", "watch"]
      # Tell KSM to scrape flux conditions as metrics
      customResourceState:
        enabled: true
        config:
          spec:
            # more or less verbatim from
            # https://github.com/fluxcd/flux2-monitoring-example/blob/main/monitoring/controllers/kube-prometheus-stack/kube-state-metrics-config.yaml
            resources:
            - groupVersionKind:
                group: kustomize.toolkit.fluxcd.io
                version: v1
                kind: Kustomization
              metricNamePrefix: gotk
              metrics:
              - name: "resource_info"
                help: "The current state of a Flux Kustomization resource."
                each:
                  type: Info
                  info:
                    labelsFromPath:
                      name: [ metadata, name ]
                labelsFromPath:
                  exported_namespace: [ metadata, namespace ]
                  ready: [ status, conditions, "[type=Ready]", status ]
                  suspended: [ spec, suspend ]
                  revision: [ status, lastAppliedRevision ]
                  source_name: [ spec, sourceRef, name ]
                  # extra labels so we can link to canonical sources from dashboards
                  cx_path: [ metadata, annotations, "cx.dashboards/path"]
                  cx_repo: [ metadata, annotations, "cx.dashboards/repo"]

            - groupVersionKind:
                group: source.toolkit.fluxcd.io
                version: v1
                kind: GitRepository
              metricNamePrefix: gotk
              metrics:
              - name: "resource_info"
                help: "The current state of a Flux GitRepository resource."
                each:
                  type: Info
                  info:
                    labelsFromPath:
                      name: [ metadata, name ]
                labelsFromPath:
                  exported_namespace: [ metadata, namespace ]
                  ready: [ status, conditions, "[type=Ready]", status ]
                  suspended: [ spec, suspend ]
                  revision: [ status, artifact, revision ]
                  url: [ spec, url ]
                  # extra label so we can link to canonical sources from dashboards
                  cx_path: [ metadata, annotations, "cx.dashboards/path"]
                  cx_repo: [ metadata, annotations, "cx.dashboards/repo"]

            - groupVersionKind:
                group: source.toolkit.fluxcd.io
                version: v1beta2
                kind: OCIRepository
              metricNamePrefix: gotk
              metrics:
              - name: "resource_info"
                help: "The current state of a Flux OCIRepository resource."
                each:
                  type: Info
                  info:
                    labelsFromPath:
                      name: [ metadata, name ]
                labelsFromPath:
                  exported_namespace: [ metadata, namespace ]
                  ready: [ status, conditions, "[type=Ready]", status ]
                  suspended: [ spec, suspend ]
                  revision: [ status, artifact, revision ]
                  url: [ spec, url ]
                  # extra label so we can link to canonical sources from dashboards
                  cx_path: [ metadata, annotations, "cx.dashboards/path"]
                  cx_repo: [ metadata, annotations, "cx.dashboards/repo"]

      # Opt-out of collectors we do not care about metrics for
      # https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-state-metrics/values.yaml#L348C1-L377C1
      collectors:
      #- certificatesigningrequests
      #- configmaps
      - cronjobs
      - daemonsets
      - deployments
      #- endpoints
      - horizontalpodautoscalers
      #- ingresses
      - jobs
      #- leases
      #- limitranges
      - mutatingwebhookconfigurations
      - namespaces
      #- networkpolicies
      - nodes
      - persistentvolumeclaims
      - persistentvolumes
      #- poddisruptionbudgets
      - pods
      - replicasets
      #- replicationcontrollers
      #- resourcequotas
      #- secrets
      #- services
      - statefulsets
      - storageclasses
      - validatingwebhookconfigurations
      - volumeattachments

      prometheus:
        monitor:
          enabled: true
          metricRelabelings:
          # Drop less useful pod metrics (5x per pod..)
          - sourceLabels: [__name__]
            regex: 'kube_pod_status_reason'
            action: drop
          - sourceLabels: [__name__]
            regex: 'kube_pod_tolerations|kube_pod_status_qos_class|kube_pod_scheduler|kube_pod_service_account|kube_pod_status_scheduled_time'
            action: drop
          - sourceLabels: [__name__]
            regex: 'kube_replicaset_(status|spec|metadata|created).*'
            action: drop

    grafana:
      enabled: false
      forceDeployDashboards: true
      defaultDashboardsEnabled: true
      serviceMonitor:
        enabled: true
        metricRelabelings:
        - sourceLabels: [__name__]
          regex: '(grafana_http_|grafana_live_|grafana_frontend_|grafana_access|grafana_alerting|grafan_folder|grafana_plugin|grafana_feature_toggles).*'
          action: drop
        - sourceLabels: [__name__]
          regex: 'grafana.*_bucket'
          action: drop
        - sourceLabels: [__name__]
          action: drop
          regex: '^go_.*'
      datasource:
        enabled: true
        default: true
        httpMethod: "POST"
        scrapeInterval: "30s"
        uid: "prometheus"
    kubeApiServer:
      enabled: false
      serviceMonitor:
        enabled: false
    kubelet:
      enabled: true
      serviceMonitor:
        # we want cadvisor (cpu/memory) metrics only
        probes: false
        cAdvisor: true
        cAdvisorMetricRelabelings:
        - sourceLabels: [__name__]
          action: drop
          regex: scheduler_.*
        - sourceLabels: [__name__]
          action: drop
          regex: apiserver_.*
        # Default drops from KPS
        # Drop less useful container CPU metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
        # Drop less useful container / always zero filesystem metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
        # Drop less useful / always zero container memory metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_memory_(mapped_file|swap)'
        # Drop less useful container process metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_(file_descriptors|tasks_state|threads_max)' # could drop _sockets and _processes here but it's interesting
        # Drop container spec metrics that overlap with kube-state-metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_spec.*'
        # Drop cgroup metrics with no pod.
        - sourceLabels: [id, pod]
          action: drop
          regex: '.+;'
        - regex: 'interface|container'
          action: labeldrop
        # END DEFAULT DROPS
        # Extra drops
        # Drop less useful container metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_(memory_failures|last_seen|memory_kernel|memory_failcnt|blkio_device|oom_events|start_time|threads|ulimits_soft).*'
        metricRelabelings:
        - action: replace
          sourceLabels: [node]
          targetLabel: instance
        # Drop almost all kubelet metrics, only want cpu/memory usage
        - sourceLabels: [__name__]
          regex: '(apiserver_|etcd_|workqueue_|kubernetes_feature_|endpoint_slice_|node_authorizer_|storage_|kubeproxy_|aggregator_|authentication_|scheduler_|apiextensions_|rest_client_).*'
          action: drop
        # more metrics from kubelet
        - sourceLabels: [__name__]
          regex: '(job_controller_|cronjob_controller_|authorization_|field_validation_).*'
          action: drop
        # Drop kubelet_ metrics except kubelet_volume containing usage data for PVCs
        - sourceLabels: [__name__]
          action: drop
          regex: '(kubelet_runtime_|kubelet_http_|kubelet_pod_|kube_router|kubelet_evented_|kubelet_topology_|kubelet_pleg_|kubelet_node_startup_).*'
        - sourceLabels: [__name__]
          action: drop
          regex: '^go_.*'
        # DROP ALL BUCKETS (not used in anything except kubelet dashboard)
        - sourceLabels: [__name__]
          regex: '.*bucket.*'
          action: drop
        - sourceLabels: [__name__]
          regex: '(watch_cache_capacity|volume_operation_).*'
          action: drop
    coreDns:
      enabled: false
    kubeDns:
      enabled: false
    kubeEtcd:
      service:
        selector:
          component: kube-apiserver # etcd runs on control plane nodes
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeControllerManager:
      enabled: false
    prometheus:
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          cert-manager.io/cluster-issuer: domain-0-le-prod
          cert-manager.io/private-key-rotation-policy: Always
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
          traefik.ingress.kubernetes.io/router.tls: "true"
          traefik.ingress.kubernetes.io/router.middlewares: traefik-chain-basic@kubernetescrd
        hosts:
          - prom.${DOMAIN_0}
        paths:
          - /
        pathType: Prefix
        tls:
          - hosts:
            - prom.${DOMAIN_0}
            secretName: prometheus-tls-0
      serviceMonitor:
        metricRelabelings:
        - sourceLabels: [__name__]
          regex: 'prometheus_http_(request|response).*'
          action: drop
        - sourceLabels: [__name__]
          regex: 'prometheus_tsdb_(compaction_chunk|compaction_duration).*'
          action: drop
        - sourceLabels: [__name__]
          regex: '(net_conntrack).*'
          action: drop
        - sourceLabels: [__name__]
          regex: 'prometheus_sd_(consul|azure|kuma|nomad|linode).*'
          action: drop
        - sourceLabels: [__name__]
          action: drop
          regex: '^go_.*'
      portName: main
      prometheusSpec:
        enableAPIServer: false
        serviceMonitorSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        prometheusExternalLabelNameClear: true
        replicaExternalLabelNameClear: true
        enableAdminAPI: true
        walCompression: true
        enableFeatures:
          - memory-snapshot-on-shutdown
        retention: 7d
        retentionSize: 50GB
        resources:
          requests:
            cpu: 100m
          limits:
            memory: 2000Mi
    serviceMonitor:
      enabled: false
